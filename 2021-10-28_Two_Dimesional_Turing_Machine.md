チューリングマシンは一次元のテープを持つ。これはチューリング完全性を失わない最小の次元であり、その点では妥当ではある。だが、チューリングマシンのモデルは計算可能性を念頭において設計されたものであるからにして、計算複雑性の観点では疑問がないわけでもないのだ。

> メモリのアクセスは、本来は O(√n) なんですよね。 n はメモリのサイズであるとして。
> — [Hexirp, 2021-10-22, Twitter](https://twitter.com/hexirp_prixeh/status/1451397834029219864)

かつて、こんなことを呟いた。

> "The Myth of RAM" というシリーズ記事がありまして、実際にランダムアクセスの時間を測ると O(√n) になり、また理想的なメモリでも物理学的な制約によって O(√n) が限界になると説明しています。
> https://ilikebigbits.com/2014_04_21_myth_of_ram_1.html
> — [Hexirp, 2021-10-22, Twitter](https://twitter.com/hexirp_prixeh/status/1451459195568472074)

かつて、こんなことを呟いた。

> この挙動をシミュレートするには、二次元空間のテープを持つチューリングマシンを使うことが出来る
> — [Hexirp, 2021-10-23, Twitter](https://twitter.com/hexirp_prixeh/status/1451793693107638275)

かつて、こんなことを呟いた。

今日は、これらについて考察してみようと思う。

## ランダムアクセスの計算量

ランダムアクセスの計算量は O(1) とされる。これに異議を唱えたのが "[The Myth of RAM](http://ilikebigbits.com/2014_04_21_myth_of_ram_1.html)" という記事である。ここでは結論だけを引用させていただく——ランダムアクセスの計算量は O(√n) である。

とあるメモリが存在すると考える。そのメモリの量を n とする。そのメモリへランダムアクセスした時の計算量は O(√n) となる。これ以上の性能でランダムアクセスすることは出来ない。そういうことなのだ。

ここでのメモリの量の単位はビットでもバイトでも良い。単位での違いは定数倍にしかならない。キャッシュを使えば性能は上がるかもしれないが、キャッシュは**非**ランダムアクセスを前提にしている。

どこから n の 1/2 乗という式は出てきたのか？ 記事では二つの根拠が説明されている。

一つ目は、単純な実験の結果である。 [emilk/ram_bench](https://github.com/emilk/ram_bench) のコードにより計測された結果は 1/2 乗で近似されるように見える。残念ながら、これは確実な証拠ではない。

二つ目は、物理学的な限界によるものである。まず、我々は三次元空間に生きている。よって、時間 t でアクセスできる空間は球状になり、その体積は t の 3 乗に比例する。では、この空間にどれだけの情報を詰め込めるだろうか？ その限界を物理学は与えている——ベッケンシュタイン境界である。これは球状の空間の中に存在できる情報の量の限界を与えている。その限界に達する唯一の物体はブラックホールであり、ブラックホールの情報量は半径の 2 乗に比例する。すなわち、時間 t でアクセス可能な空間の中に詰め込める情報量の限界は t の 2 乗に比例するのである。これを逆側に解いて、メモリの量が n であるとき、そのメモリにランダムアクセスする時に必要な時間は O(√n) を**下回ることは出来ない**という結果が得られる。
